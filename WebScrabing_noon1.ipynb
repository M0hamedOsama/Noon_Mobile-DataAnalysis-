{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofbzaVwYpXgg",
        "outputId": "249d463c-b537-4f38-e62f-315643130928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed page 1, found 50 products.\n",
            "Processed page 2, found 50 products.\n",
            "Processed page 3, found 50 products.\n",
            "Processed page 4, found 50 products.\n",
            "Processed page 5, found 50 products.\n",
            "Processed page 6, found 50 products.\n",
            "Processed page 7, found 49 products.\n",
            "Processed page 8, found 50 products.\n",
            "Processed page 9, found 50 products.\n",
            "Processed page 10, found 50 products.\n",
            "Processed page 11, found 50 products.\n",
            "Processed page 12, found 50 products.\n",
            "Processed page 13, found 50 products.\n",
            "Processed page 14, found 50 products.\n",
            "Processed page 15, found 50 products.\n",
            "Processed page 16, found 6 products.\n",
            "No products found, stopping.\n",
            "Links exported to noon_mobile_links.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://www.noon.com/egypt-en/electronics-and-mobiles/mobiles-and-accessories/mobiles-20905/eg-all-mobiles/\"\n",
        "\n",
        "# Initialize lists to store scraped data\n",
        "product_links = []\n",
        "\n",
        "# Pagination loop\n",
        "page = 1\n",
        "\n",
        "while True:\n",
        "    # Request the page\n",
        "    url = f\"{base_url}?page={page}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error on page {page}: {e}\")\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all product containers\n",
        "    products = soup.find_all('div', class_='sc-57fe1f38-0 eSrvHE')\n",
        "\n",
        "    if not products:\n",
        "        print(\"No products found, stopping.\")\n",
        "        break\n",
        "    for product in products:\n",
        "        # Extract product link\n",
        "        link = product.find('a', href=True)\n",
        "        if link:\n",
        "            full_link = f\"https://www.noon.com{link['href']}\"\n",
        "            product_links.append(full_link)\n",
        "\n",
        "    print(f\"Processed page {page}, found {len(products)} products.\")\n",
        "\n",
        "    # Increment page count\n",
        "    page += 1\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Product Link': product_links\n",
        "})\n",
        "\n",
        "df.to_csv('noon_mobile_links.csv', index=False)\n",
        "print(\"Links exported to noon_mobile_links.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "\n",
        "# Load the CSV file with URLs\n",
        "file_path = '/content/noon_mobile_links.csv'\n",
        "urls_df = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare an empty list to collect data\n",
        "all_data = []\n",
        "\n",
        "\n",
        "# Iterate over each URL in the DataFrame with tqdm progress bar\n",
        "for url in tqdm(urls_df['Product Link'], desc=\"Scraping Products\", unit=\"product\"):\n",
        "    try:\n",
        "        # Send a GET request to the webpage\n",
        "\n",
        "        # Render the JavaScript on the page (this might take a couple of seconds)\n",
        "        # Increased timeout to 20 seconds\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract the title\n",
        "        title = soup.find('h1', class_='sc-b5e5edab-22 cRYLTe').text.strip() if soup.find('h1', class_='sc-b5e5edab-22 cRYLTe') else pd.NA\n",
        "\n",
        "        # Extract the price\n",
        "        price = soup.find('div', class_='priceNow').text.strip() if soup.find('div', class_='priceNow') else pd.NA\n",
        "\n",
        "        # Extract brand\n",
        "        brand = soup.find('div', class_='sc-b5e5edab-21 jjBJcR').text.strip() if soup.find('div', class_='sc-b5e5edab-21 jjBJcR') else pd.NA\n",
        "\n",
        "        # Extract rating\n",
        "        rating = soup.find('div', class_='sc-9cb63f72-2 dGLdNc').text if soup.find('div', class_='sc-9cb63f72-2 dGLdNc') else pd.NA\n",
        "\n",
        "        # Extract product image\n",
        "        img_tags = soup.find_all('img')\n",
        "\n",
        "    # Extract the 'src' attribute from each <img> tag and clean it\n",
        "        product_image = []\n",
        "        for img in img_tags:\n",
        "            img_url = img.get('src')\n",
        "            if img_url and img_url.startswith('https://f.nooncdn.com/') and img_url.endswith('width=240'):\n",
        "                # Remove query parameters from the URL (anything after '?')\n",
        "                clean_img_url = img_url.split('?')[0]\n",
        "                product_image.append(clean_img_url)\n",
        "\n",
        "        # Extract the specifications table\n",
        "        specifications = {}\n",
        "        specs_table = soup.find('table').find('tbody').find_all('tr') if soup.find('table') else []\n",
        "\n",
        "        for row in specs_table:\n",
        "            # Get the header (first column) and value (second column)\n",
        "            header = row.find_all('td')[0].text.strip()\n",
        "            value = row.find_all('td')[1].text.strip()\n",
        "            specifications[header] = value\n",
        "\n",
        "        # Add title, price, and availability to specifications\n",
        "        specifications.update({\n",
        "            'Title': title,\n",
        "            'Price': price,\n",
        "            'Brand': brand,\n",
        "            'Rating': rating,\n",
        "            'Product Image': product_image,\n",
        "            'Product Link': url\n",
        "        })\n",
        "\n",
        "        # Append the specifications as a row to the list\n",
        "        all_data.append(specifications)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "specs_df = pd.DataFrame(all_data)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "specs_df.to_csv('/content/noon_mobile_links.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htsCRxm0plTh",
        "outputId": "31460f4e-0f16-46be-ec24-d9b8f76e1895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping Products: 100%|██████████| 806/806 [10:30<00:00,  1.28product/s]\n"
          ]
        }
      ]
    }
  ]
}